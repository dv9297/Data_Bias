# Data_Bias

Analysis on labeled_and_scored_comments:
In order to process and analyze the scores given to the comments I observed the 50 largest and smallest scores from labeled_and_scored_comments. I also observed some standard basic stats from the overall column 'score'. I wanted to look at the extremes in case they were any outliers within the extremes. I realized that while I was doing this that there was not a tally('Total_Score') on the rate/type of toxicity. Since the tally chas a direct effect on the score I decided to create a new column that calculated the tally per row. I wanted to see if the higher the tally the higher the score and vice versa. When I performed this on the wikipedia comments, I noticed that the majority of row with higher tallies had higher score but there were some outlier in rows with the highest columns that showed that a row with a tally count of 2 can also have a score of 0.63 which is pretty high when the mean of the score column was 0.24. The same applied to the rows with the smallest score who had a tally of 0 yet had a score 0.64. I also performed several tests on the specific tally score and obtained basis stats to see the extremes of that score, and there were several uncorrelated scores, seperating into groups in my code allowed me to see the errors a lot better. For every tally amount I got the top 10 smallest and largest rows. There were wrongly categorized comments who deserved higher scores in tallys with lower amounts, and scores with higher tallyes deserved lower scores of toxicity. All that to say that the toxicity function was not always right and very prone to errors.

Biases:
Some of the biases that I think exist are that the system can determine the score based on words associated with a negative connotation. The system may not comprehend context and therefore give the comment a high score of toxicity when it was not, despite the tally rates of the comments being 0. For example 'Dear god this site is horrible' was rated 0.45, when in reality this does not deserve that high of a score and the tally rate being 0. Then again there were some tally scores that were 1/6 and had a score of 0.98 and definitely was categorized correctly. I am not sure if the tally score really matters or if each type of toxic rate has a certain weight to it. There is also a possible bias of the function not being able to process certain words because people use special characters to write hateful comments. 

Personal Analysis:
Problem: 
In the twitter community there is alway a lot of controversy between K-Pop Fans who stan different groups. With big fandoms such as with the K-Pop band BTS, there is a lot of rivalry between fans and haters. Fans of BTS usually refer to themselves as K-Army and those who hate BTS are referred to as anti-armys. The dilemma that often comes about is that anti-army hate comments are a lot harder to take down and oftentimes K-Army positive comments are taken down more often than the comments from anti-army. 
Hypothesis: Based on the misscoring of the previous wikipedia comments data set, I predict that anti-army comments will be scored lower than k-army comments. I think these are the results that will be yielded because anti-army people usually use special characters and use clever ways to work around systems that identify negative language so that their hate can continue to spread. I think k-army will receive higher scores than anti-army because they are usually describing how attractive the members are, sometimes although they are complimenting them there are words that can be triggering for the system. This was seen in the 'Total_Score' Column that had a tally of 0 and had high scores because a word was taken out of context.
Methods:
I chose 12 comments from twitter from different threads commenting on namjoons appearance. I sourced (links attached on doc) the twitter comments and chose 6 positive comments from k-army and 6 negative comments from ant-army. I made 2 different lists of these comments and then put them through the scoring function to receive a score for each comment. I then decided to categorize the scoring of the comments by priority using a set threshold. These thresholds were obtained by getting the score column general stats and analyzing the trend score of the wikipedia comments. The maximum threshold was set at 0.84 because comments at or above this score usually were rank with higher tallies ‘total-scores’ and after being reviewed were pretty accurate to high levels of toxicity, so anything above that was considered highly priority scores .The minimum threshold was obtained from general stats at 000004 and the medium threshold was obtained by making observations between comments with a ‘total_score’ row total of 4 and above and see what was in between that and the maximum score. The medium score of 0.12 was a common score for comments that were not that toxic, although it was lower than the 50% threshold of the general stats from the column. In order to make it easier to understand the categorization of the threshold I created a simple for loop in order to label the different priority scores for the comments. 

Results:
The results demonstrated that the majority of the anti-comments were labeled at a max priority score, which means that the scores were really toxic. K-army tweet comments were mainly scored as Medium priority. 

Interpretation of results:
I think the results yielded were really surprising because I thought that the score function would not be able to detect the negative words used in special characters but I was wrong. The negative comments were scored for the most part accurately. With k-army I think they were rated higher than they should have, most were rated at a medium priority and I think they were higher than they should have been. The reason they might have been like that is because I set a large range between the medium threshold. There were some trigger words in the good comments like ‘sexy’ might have led to the score being higher. Those words were also repeated many times which could have added to the score. I dont think that lowering the threshold would have made it any better because then there will be some suceptible to errors. There might be some biashand since I am attached to the subject I might think comments are positive but from an outsider perspective they might be to intimate and make them toxic because some of the members might find it offensive that they are beeing seeing as that or in that way. 
 
